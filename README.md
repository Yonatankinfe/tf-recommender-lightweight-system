# tf-recommender-lightweight-system
A lightweight content-based recommendation system built with Python and TensorFlow/Keras, exposed via a simple REST API with automatic API key generation.
# Content-Based Recommendation System

This project implements a lightweight, simple content-based recommendation system using Python. It leverages TF-IDF for feature extraction, TensorFlow/Keras for generating neural network embeddings, and FastAPI for serving a RESTful API.

## üåü Features

*   **Content-Based Recommendations**: Recommends items similar to a given item based on their content (e.g., title, description, tags).
*   **Neural Network Embeddings**: Uses a simple Multi-Layer Perceptron (MLP), trained in an autoencoder-like fashion on TF-IDF features, to generate item embeddings.
*   **TF-IDF**: Utilizes Term Frequency-Inverse Document Frequency for initial text feature extraction.
*   **RESTful API**: Exposes recommendation functionality via a FastAPI application.
*   **API Key Authentication**: Secures the recommendation endpoint using API keys.
*   **Automatic Key Generation**: Includes an endpoint to generate new API keys, which are stored in a `.env` file.

## üìÇ Project Structure
```bash
recommendation_system/
‚îú‚îÄ‚îÄ data/                            # Contains input datasets
‚îÇ   ‚îî‚îÄ‚îÄ items.csv                    # Example item data (item_id, title, description)
‚îÇ
‚îú‚îÄ‚îÄ models/                          # Trained models and saved artifacts
‚îÇ   ‚îú‚îÄ‚îÄ tfidf_vectorizer.pkl         # Saved TF-IDF vectorizer
‚îÇ   ‚îú‚îÄ‚îÄ embedding_model.keras        # Trained Keras embedding model
‚îÇ   ‚îî‚îÄ‚îÄ item_embeddings.pkl          # Pickled item embeddings
‚îÇ
‚îú‚îÄ‚îÄ src/                             # Source code for the application
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py                  # Package initializer
‚îÇ   ‚îú‚îÄ‚îÄ auth.py                      # API key handling and authentication
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py               # Loads and prepares item data
‚îÇ   ‚îú‚îÄ‚îÄ main.py                      # FastAPI application entry point
‚îÇ   ‚îú‚îÄ‚îÄ model.py                     # Defines the Keras model
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py             # Text preprocessing and TF-IDF logic
‚îÇ   ‚îú‚îÄ‚îÄ recommender.py               # Core recommendation logic (e.g., cosine similarity)
‚îÇ   ‚îú‚îÄ‚îÄ train.py                     # Model training and artifact generation
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                     # Utility functions (e.g., saving/loading artifacts)
‚îÇ
‚îú‚îÄ‚îÄ venv/                            # Python virtual environment (recommended)
‚îÇ
‚îú‚îÄ‚îÄ .env                             # Environment variables (e.g., API keys)
‚îú‚îÄ‚îÄ README.md                        # Project documentation
‚îî‚îÄ‚îÄ requirements.txt                 # Python dependencies
```

*   `data/`: Contains the raw data for items.
*   `models/`: Stores the pre-trained TF-IDF vectorizer, the Keras embedding model, and the generated item embeddings.
*   `src/`: Contains all the Python source code for the application.

## üöÄ Setup

1.  **Clone the Repository (or place project files):**
    ```bash
    # If you have the project folder named 'recommendation_system'
    cd recommendation_system
    ```

2.  **Create and Activate a Virtual Environment:**
    (Recommended to keep dependencies isolated)
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install Dependencies:**
    Ensure you have a `requirements.txt` file with all necessary packages.
    ```bash
    pip install -r requirements.txt
    ```
    *(If `requirements.txt` is not provided, you'll need to create it, e.g., `pip freeze > requirements.txt` after installing packages like `tensorflow`, `scikit-learn`, `fastapi`, `uvicorn`, `python-dotenv`, `pandas`)*

## üèãÔ∏è Training the Model

Run the training script from the project's root directory to process the data, train the embedding model, and generate the necessary artifacts in the `models/` directory.

```bash
python src/train.py
```

### This script performs the following steps:
* Loads data from data/items.csv.
* Preprocesses the text content (e.g., title, description).
* Creates and fits a TF-IDF vectorizer.
* Builds and trains the neural network model (autoencoder-style) to learn item embeddings.
* Generates and saves embeddings for all items using the trained base model.
* Saves the TF-IDF vectorizer, the Keras embedding model, and the final item embeddings to the models/ directory.
